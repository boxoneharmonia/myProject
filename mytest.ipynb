{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4a81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.net import EventBERT\n",
    "from config import config\n",
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001d978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 初始化配置 ---\n",
      "--- 实例化 Backbone 模型 ---\n",
      "\n",
      "--- 准备模拟输入数据 ---\n",
      "\n",
      "--- 执行模型前向传播 ---\n",
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "EventBERT                                                    [1, 24, 12]               184,320\n",
      "├─EventImg2Token: 1-1                                        [1, 216, 768]             --\n",
      "│    └─Sequential: 2-1                                       [24, 1536, 7, 7]          --\n",
      "│    │    └─ConvDw: 3-1                                      [24, 32, 200, 200]        150\n",
      "│    │    └─ConvDw: 3-2                                      [24, 64, 100, 100]        2,528\n",
      "│    │    └─ConvDw: 3-3                                      [24, 128, 50, 50]         9,152\n",
      "│    │    └─ConvDw: 3-4                                      [24, 512, 25, 25]         67,968\n",
      "│    │    └─ConvDw: 3-5                                      [24, 1024, 13, 13]        531,968\n",
      "│    │    └─ConvDw: 3-6                                      [24, 1536, 7, 7]          1,587,200\n",
      "│    └─MLP_base: 2-2                                         [1, 216, 768]             --\n",
      "│    │    └─Linear: 3-7                                      [1, 216, 3072]            4,721,664\n",
      "│    │    └─Identity: 3-8                                    [1, 216, 3072]            --\n",
      "│    │    └─Sequential: 3-9                                  [1, 216, 768]             2,366,208\n",
      "├─Transformer: 1-2                                           [1, 216, 768]             166,656\n",
      "│    └─Sequential: 2-3                                       [1, 217, 768]             --\n",
      "│    │    └─Block: 3-10                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-11                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-12                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-13                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-14                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-15                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-16                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-17                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-18                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-19                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-20                                      [1, 217, 768]             7,094,016\n",
      "│    │    └─Block: 3-21                                      [1, 217, 768]             7,094,016\n",
      "│    └─Sequential: 2-4                                       [1, 217, 768]             --\n",
      "│    │    └─MLP_base: 3-22                                   [1, 217, 768]             1,182,720\n",
      "│    │    └─LayerNormCompatible: 3-23                        [1, 217, 768]             1,536\n",
      "├─MLP_base: 1-3                                              [1, 24, 768]              --\n",
      "│    └─Linear: 2-5                                           [1, 24, 768]              5,376\n",
      "│    └─Identity: 2-6                                         [1, 24, 768]              --\n",
      "│    └─Sequential: 2-7                                       [1, 24, 768]              --\n",
      "│    │    └─LayerNormCompatible: 3-24                        [1, 24, 768]              1,536\n",
      "│    │    └─GELU: 3-25                                       [1, 24, 768]              --\n",
      "│    │    └─Linear: 3-26                                     [1, 24, 768]              590,592\n",
      "├─Sequential: 1-4                                            [1, 240, 768]             --\n",
      "│    └─Block: 2-8                                            [1, 240, 768]             --\n",
      "│    │    └─LayerNormCompatible: 3-27                        [1, 240, 768]             1,536\n",
      "│    │    └─Attention: 3-28                                  [1, 240, 768]             2,362,368\n",
      "│    │    └─Identity: 3-29                                   [1, 240, 768]             --\n",
      "│    │    └─LayerNormCompatible: 3-30                        [1, 240, 768]             1,536\n",
      "│    │    └─MLP_base: 3-31                                   [1, 240, 768]             4,728,576\n",
      "│    │    └─Identity: 3-32                                   [1, 240, 768]             --\n",
      "├─Linear: 1-5                                                [1, 24, 12]               9,228\n",
      "==============================================================================================================\n",
      "Total params: 103,651,010\n",
      "Trainable params: 103,651,010\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.27\n",
      "==============================================================================================================\n",
      "Input size (MB): 7.68\n",
      "Forward/backward pass size (MB): 1656.47\n",
      "Params size (MB): 413.20\n",
      "Estimated Total Size (MB): 2077.35\n",
      "==============================================================================================================\n",
      "\n",
      "--- 测试完成 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 初始化配置 ---\")\n",
    "\n",
    "print(\"--- 实例化 Backbone 模型 ---\")\n",
    "model = EventBERT(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\n--- 准备模拟输入数据 ---\")\n",
    "batch_size = 1\n",
    "seq_len = config.max_seq_len # 确保 seq_len <= config.max_seq_len\n",
    "input_channels = 2\n",
    "input_height = 200\n",
    "input_width = 200\n",
    "\n",
    "# 模拟正负事件图像序列\n",
    "# (batch_size, seq_len, C, H, W)\n",
    "mock_x = torch.randn(batch_size, seq_len, input_channels, input_height, input_width).to(device)\n",
    "mock_traj = torch.randn(batch_size, seq_len, 6).to(device)\n",
    "\n",
    "print(\"\\n--- 执行模型前向传播 ---\")\n",
    "summary(model, input_data=[mock_x, mock_traj], verbose=1)\n",
    "\n",
    "print(\"\\n--- 测试完成 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd25e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del mock_x \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d38abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始从 DataLoader 中取出一个批次的数据 ---\n",
      "成功取出一个批次！\n",
      "x_seq 的形状: torch.Size([1, 24, 2, 200, 200])\n",
      "traj_seq 的形状: torch.Size([1, 24, 12])\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from src.dataset import build_dataloader\n",
    "\n",
    "config.task = 'traj'\n",
    "config.batch_size = 1\n",
    "\n",
    "dataloader = build_dataloader(config)\n",
    "\n",
    "# 实例化 DataLoader\n",
    "\n",
    "print(\"\\n--- 开始从 DataLoader 中取出一个批次的数据 ---\")\n",
    "\n",
    "# 从迭代器中获取一个批次\n",
    "try:\n",
    "    x_seq, traj_seq = next(iter(dataloader))\n",
    "    \n",
    "    print(f\"成功取出一个批次！\")\n",
    "    print(f\"x_seq 的形状: {x_seq.shape}\")\n",
    "    print(f\"traj_seq 的形状: {traj_seq.shape}\")\n",
    "\n",
    "        \n",
    "except StopIteration:\n",
    "    print(\"错误： DataLoader 为空，无法取出数据。请检查数据集路径和内容。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import torch\n",
    "print(accelerate.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.bfloat16)\n",
    "print(a, a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def KL(p,q):\n",
    "    # p,q 为两个 list，表示对应取值的概率 且 sum(p) == 1 ，sum(q) == 1\n",
    "    return sum(_p*math.log(_p/_q) for (_p,_q) in zip(p,q) if _p != 0 )\n",
    "\n",
    "P = [0.2, 0.4, 0.4]\n",
    "Q = [0.2, 0.4, 0.4]\n",
    "\n",
    "print(KL(P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from src.net import build_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# config.use_cuda = False\n",
    "\n",
    "if torch.cuda.is_available() and config.use_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_num_threads(8) \n",
    "\n",
    "model = build_model(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "weight_path = os.path.join(config.weight_dir, \"event_bert_mlm\" + \".pth\")\n",
    "if os.path.exists(weight_path):\n",
    "    model.load_state_dict(torch.load(weight_path, map_location='cpu'), strict=False)\n",
    "    logger.info(f\"Loading pretrained weights from {weight_path}\")\n",
    "else:\n",
    "    logger.warning(f\"Pretrained weights not found at {weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "captured_outputs = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    一个前向钩子函数。\n",
    "    每次模块(module)的forward被调用后，此函数会被触发。\n",
    "    'output' 就是我们想要的 eventImg2Token 的输出结果。\n",
    "    \"\"\"\n",
    "    # 我们将输出从GPU移到CPU并分离计算图，以防内存泄漏\n",
    "    captured_outputs.append(output.detach().cpu())\n",
    "\n",
    "target_module = model.eventImg2Token\n",
    "handle = target_module.register_forward_hook(forward_hook)    \n",
    "\n",
    "num_test_loops = 20  # 我们测试20个不同的随机输入\n",
    "with torch.no_grad(): # 确保不计算梯度\n",
    "    for _ in tqdm(range(num_test_loops), desc=\"Diagnosing Collapse\"):\n",
    "        # 在每次循环中创建全新的随机输入\n",
    "        mock_x_pos = torch.randn(1, config.max_seq_len, 3, 200, 200) *2 -1\n",
    "        mock_x_neg = torch.randn(1, config.max_seq_len, 3, 200, 200) *2 -1\n",
    "        \n",
    "        # 正常调用模型。当我们调用它时，钩子会自动被触发\n",
    "        model(mock_x_pos.to(device), mock_x_neg.to(device), config.mask_probability)\n",
    "\n",
    "# --- 分析捕获到的数据 ---\n",
    "if captured_outputs:\n",
    "    # 将所有捕获到的输出张量拼接在一起\n",
    "    # 每个输出的形状是 (1, S, token_len)，拼接后是 (num_test_loops, S, token_len)\n",
    "    all_outputs = torch.cat(captured_outputs, dim=0)\n",
    "\n",
    "    # 对logits进行softmax，转换为概率分布\n",
    "    all_probs = torch.softmax(all_outputs, dim=-1)\n",
    "\n",
    "    # 计算在“样本”维度(dim=0)上的标准差\n",
    "    stdev_across_samples = all_probs.std(dim=0)\n",
    "    mean_stdev = stdev_across_samples.mean().item()\n",
    "\n",
    "    logger.info(f\"--- Diagnosis Result ---\")\n",
    "    logger.info(f\"Mean Standard Deviation of softmax outputs across {num_test_loops} random inputs: {mean_stdev:.8f}\")\n",
    "\n",
    "    if mean_stdev < 1e-4: # 一个非常低的阈值\n",
    "        logger.error(\"!!! CRITICAL: Representation Collapse DETECTED.\")\n",
    "        logger.info(\"The model produces nearly identical probability distributions for different random inputs.\")\n",
    "        \n",
    "        # 亲眼看看证据\n",
    "        logger.info(\"Displaying softmax probability of the first token from the first 3 random inputs:\")\n",
    "        for i in range(min(3, num_test_loops)):\n",
    "            # 打印第一个样本，第一个token，前10个值的概率\n",
    "            print(f\"Random Input {i+1}: {all_probs[i, 0, :10]}...\")\n",
    "    else:\n",
    "        logger.info(\"OK: No obvious sign of representation collapse. The model produces diverse outputs.\")\n",
    "else:\n",
    "    logger.warning(\"Hook did not capture any outputs.\")\n",
    "\n",
    "# --- 4. 事后清理：移除钩子 ---\n",
    "# 这是一个好习惯，防止钩子在后续代码中继续产生影响\n",
    "handle.remove()\n",
    "logger.info(\"Hook removed.\")\n",
    "\n",
    "del model\n",
    "del mock_x_pos\n",
    "del mock_x_neg \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "num_vectors = all_outputs.size(0) * all_outputs.size(1)\n",
    "token_dimension = all_outputs.size(2)\n",
    "\n",
    "all_vectors_flat = all_outputs.view(num_vectors, token_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0606e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_tsne(vectors: torch.Tensor):\n",
    "    \"\"\"\n",
    "    使用t-SNE对向量进行二维可视化。\n",
    "    \"\"\"\n",
    "    logger.info(\"--- t-SNE Visualization ---\")\n",
    "    logger.info(\"Running t-SNE... this may take a moment.\")\n",
    "    \n",
    "    # t-SNE需要numpy数组\n",
    "    vectors_np = vectors.cpu().numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=15, learning_rate='auto', init='random')\n",
    "    vectors_2d = tsne.fit_transform(vectors_np)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6)\n",
    "    plt.title(\"t-SNE Visualization of Output Vectors\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    # 保存图像而不是显示\n",
    "    # plt.savefig(\"tsne_visualization.png\")\n",
    "    # logger.info(\"t-SNE visualization saved to tsne_visualization.png\")\n",
    "\n",
    "visualize_tsne(all_vectors_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_cosine_similarity(vectors: torch.Tensor):\n",
    "    \"\"\"\n",
    "    分析一批向量的平均余弦相似度。\n",
    "    Args:\n",
    "        vectors (torch.Tensor): 形状为 (N, D) 的向量，N是样本数，D是向量维度。\n",
    "    \"\"\"\n",
    "    # 1. 对向量进行L2归一化\n",
    "    vectors_normalized = F.normalize(vectors, p=2, dim=1)\n",
    "    \n",
    "    # 2. 计算所有成对的余弦相似度\n",
    "    # (N, D) x (D, N) -> (N, N) 的相似度矩阵\n",
    "    cosine_matrix = torch.matmul(vectors_normalized, vectors_normalized.t())\n",
    "    \n",
    "    # 3. 提取上三角部分（不包括对角线），计算平均值\n",
    "    # 对角线上的值总是1（自己和自己的相似度），需要排除\n",
    "    N = vectors.size(0)\n",
    "    # 创建一个上三角掩码\n",
    "    mask = torch.triu(torch.ones(N, N), diagonal=1).bool()\n",
    "    # 提取所有成对相似度值\n",
    "    pairwise_similarities = cosine_matrix[mask]\n",
    "    \n",
    "    # 4. 计算平均值和标准差\n",
    "    avg_cosine_sim = pairwise_similarities.mean().item()\n",
    "    std_cosine_sim = pairwise_similarities.std().item()\n",
    "\n",
    "    logger.info(f\"--- Cosine Similarity Analysis ---\")\n",
    "    logger.info(f\"Average pairwise cosine similarity: {avg_cosine_sim:.6f}\")\n",
    "    logger.info(f\"Std dev of cosine similarity: {std_cosine_sim:.6f}\")\n",
    "\n",
    "    if avg_cosine_sim > 0.99: # 设置一个非常高的阈值\n",
    "        logger.error(\"!!! CRITICAL: Representation Collapse DETECTED via Cosine Similarity.\")\n",
    "        logger.error(\"Vectors are pointing in nearly the same direction.\")\n",
    "    else:\n",
    "        logger.info(\"OK: Cosine similarity analysis shows no sign of collapse.\")\n",
    "\n",
    "analyze_cosine_similarity(all_vectors_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from transformers.optimization import get_scheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "tensor = torch.FloatTensor([[4]]) # build a tensor\n",
    "x = Variable(tensor, requires_grad=True) # build a variable, usually for compute gradients\n",
    "\n",
    "scheduler_name = 'polynomial'\n",
    "scheduler_specific_kwargs = {}\n",
    "if scheduler_name == 'cosine_with_restarts':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'num_cycles': 2,\n",
    "    }\n",
    "elif scheduler_name == 'cosine_with_min_lr':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'min_lr': 0.1\n",
    "    }\n",
    "elif scheduler_name == 'polynomial':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'power': 2.5\n",
    "    }\n",
    "steps = 100\n",
    "warmup = 0.1\n",
    "optimizer2 = torch.optim.Adam([x], lr=0.1)\n",
    "scheduler2 = get_scheduler(\n",
    "        name=scheduler_name,\n",
    "        optimizer=optimizer2,\n",
    "        num_warmup_steps=int(steps*warmup),\n",
    "        num_training_steps=steps*2,\n",
    "        scheduler_specific_kwargs=scheduler_specific_kwargs\n",
    ")\n",
    "\n",
    "learning_rates2 = []\n",
    "\n",
    "for step in range(steps):\n",
    "    learning_rates2.append(optimizer2.param_groups[0]['lr'])\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    outputs2 = torch.pow((x - 5), 2)\n",
    "    loss2 = outputs2\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Plotting the learning rates\n",
    "plt.plot(range(steps), learning_rates2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(scheduler_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc0e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import config\n",
    "import os\n",
    "weight_path = os.path.join(config.weight_dir, \"event_bert_mlm.pth\")\n",
    "state_dict = torch.load(weight_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba40764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token \t torch.Size([1, 768])\n",
      "eventImg2Token.conv.0.conv.0.weight \t torch.Size([2, 1, 3, 3])\n",
      "eventImg2Token.conv.0.conv.1.weight \t torch.Size([2])\n",
      "eventImg2Token.conv.0.conv.1.bias \t torch.Size([2])\n",
      "eventImg2Token.conv.0.conv.1.running_mean \t torch.Size([2])\n",
      "eventImg2Token.conv.0.conv.1.running_var \t torch.Size([2])\n",
      "eventImg2Token.conv.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.0.conv.3.weight \t torch.Size([32, 2, 1, 1])\n",
      "eventImg2Token.conv.0.conv.4.weight \t torch.Size([32])\n",
      "eventImg2Token.conv.0.conv.4.bias \t torch.Size([32])\n",
      "eventImg2Token.conv.0.conv.4.running_mean \t torch.Size([32])\n",
      "eventImg2Token.conv.0.conv.4.running_var \t torch.Size([32])\n",
      "eventImg2Token.conv.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.1.conv.0.weight \t torch.Size([32, 1, 3, 3])\n",
      "eventImg2Token.conv.1.conv.1.weight \t torch.Size([32])\n",
      "eventImg2Token.conv.1.conv.1.bias \t torch.Size([32])\n",
      "eventImg2Token.conv.1.conv.1.running_mean \t torch.Size([32])\n",
      "eventImg2Token.conv.1.conv.1.running_var \t torch.Size([32])\n",
      "eventImg2Token.conv.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.1.conv.3.weight \t torch.Size([64, 32, 1, 1])\n",
      "eventImg2Token.conv.1.conv.4.weight \t torch.Size([64])\n",
      "eventImg2Token.conv.1.conv.4.bias \t torch.Size([64])\n",
      "eventImg2Token.conv.1.conv.4.running_mean \t torch.Size([64])\n",
      "eventImg2Token.conv.1.conv.4.running_var \t torch.Size([64])\n",
      "eventImg2Token.conv.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.2.conv.0.weight \t torch.Size([64, 1, 3, 3])\n",
      "eventImg2Token.conv.2.conv.1.weight \t torch.Size([64])\n",
      "eventImg2Token.conv.2.conv.1.bias \t torch.Size([64])\n",
      "eventImg2Token.conv.2.conv.1.running_mean \t torch.Size([64])\n",
      "eventImg2Token.conv.2.conv.1.running_var \t torch.Size([64])\n",
      "eventImg2Token.conv.2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.2.conv.3.weight \t torch.Size([128, 64, 1, 1])\n",
      "eventImg2Token.conv.2.conv.4.weight \t torch.Size([128])\n",
      "eventImg2Token.conv.2.conv.4.bias \t torch.Size([128])\n",
      "eventImg2Token.conv.2.conv.4.running_mean \t torch.Size([128])\n",
      "eventImg2Token.conv.2.conv.4.running_var \t torch.Size([128])\n",
      "eventImg2Token.conv.2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.3.conv.0.weight \t torch.Size([128, 1, 3, 3])\n",
      "eventImg2Token.conv.3.conv.1.weight \t torch.Size([128])\n",
      "eventImg2Token.conv.3.conv.1.bias \t torch.Size([128])\n",
      "eventImg2Token.conv.3.conv.1.running_mean \t torch.Size([128])\n",
      "eventImg2Token.conv.3.conv.1.running_var \t torch.Size([128])\n",
      "eventImg2Token.conv.3.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.3.conv.3.weight \t torch.Size([512, 128, 1, 1])\n",
      "eventImg2Token.conv.3.conv.4.weight \t torch.Size([512])\n",
      "eventImg2Token.conv.3.conv.4.bias \t torch.Size([512])\n",
      "eventImg2Token.conv.3.conv.4.running_mean \t torch.Size([512])\n",
      "eventImg2Token.conv.3.conv.4.running_var \t torch.Size([512])\n",
      "eventImg2Token.conv.3.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.4.conv.0.weight \t torch.Size([512, 1, 3, 3])\n",
      "eventImg2Token.conv.4.conv.1.weight \t torch.Size([512])\n",
      "eventImg2Token.conv.4.conv.1.bias \t torch.Size([512])\n",
      "eventImg2Token.conv.4.conv.1.running_mean \t torch.Size([512])\n",
      "eventImg2Token.conv.4.conv.1.running_var \t torch.Size([512])\n",
      "eventImg2Token.conv.4.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.4.conv.3.weight \t torch.Size([1024, 512, 1, 1])\n",
      "eventImg2Token.conv.4.conv.4.weight \t torch.Size([1024])\n",
      "eventImg2Token.conv.4.conv.4.bias \t torch.Size([1024])\n",
      "eventImg2Token.conv.4.conv.4.running_mean \t torch.Size([1024])\n",
      "eventImg2Token.conv.4.conv.4.running_var \t torch.Size([1024])\n",
      "eventImg2Token.conv.4.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.5.conv.0.weight \t torch.Size([1024, 1, 3, 3])\n",
      "eventImg2Token.conv.5.conv.1.weight \t torch.Size([1024])\n",
      "eventImg2Token.conv.5.conv.1.bias \t torch.Size([1024])\n",
      "eventImg2Token.conv.5.conv.1.running_mean \t torch.Size([1024])\n",
      "eventImg2Token.conv.5.conv.1.running_var \t torch.Size([1024])\n",
      "eventImg2Token.conv.5.conv.1.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.conv.5.conv.3.weight \t torch.Size([1536, 1024, 1, 1])\n",
      "eventImg2Token.conv.5.conv.4.weight \t torch.Size([1536])\n",
      "eventImg2Token.conv.5.conv.4.bias \t torch.Size([1536])\n",
      "eventImg2Token.conv.5.conv.4.running_mean \t torch.Size([1536])\n",
      "eventImg2Token.conv.5.conv.4.running_var \t torch.Size([1536])\n",
      "eventImg2Token.conv.5.conv.4.num_batches_tracked \t torch.Size([])\n",
      "eventImg2Token.fcOut.fc1.weight \t torch.Size([3072, 1536])\n",
      "eventImg2Token.fcOut.fc1.bias \t torch.Size([3072])\n",
      "eventImg2Token.fcOut.fc2.0.weight \t torch.Size([3072])\n",
      "eventImg2Token.fcOut.fc2.0.bias \t torch.Size([3072])\n",
      "eventImg2Token.fcOut.fc2.2.weight \t torch.Size([768, 3072])\n",
      "eventImg2Token.fcOut.fc2.2.bias \t torch.Size([768])\n",
      "transformer.posEmbed \t torch.Size([1, 216, 768])\n",
      "transformer.clsToken \t torch.Size([1, 1, 768])\n",
      "transformer.encoder.0.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.0.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.0.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.0.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.0.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.0.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.0.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.0.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.0.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.0.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.0.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.0.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.0.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.0.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.1.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.1.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.1.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.1.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.1.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.1.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.1.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.1.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.1.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.1.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.1.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.1.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.1.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.1.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.2.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.2.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.2.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.2.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.2.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.2.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.2.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.2.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.2.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.2.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.2.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.2.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.2.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.2.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.3.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.3.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.3.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.3.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.3.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.3.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.3.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.3.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.3.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.3.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.3.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.3.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.3.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.3.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.4.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.4.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.4.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.4.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.4.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.4.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.4.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.4.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.4.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.4.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.4.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.4.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.4.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.4.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.5.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.5.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.5.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.5.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.5.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.5.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.5.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.5.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.5.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.5.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.5.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.5.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.5.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.5.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.6.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.6.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.6.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.6.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.6.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.6.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.6.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.6.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.6.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.6.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.6.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.6.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.6.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.6.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.7.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.7.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.7.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.7.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.7.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.7.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.7.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.7.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.7.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.7.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.7.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.7.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.7.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.7.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.8.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.8.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.8.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.8.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.8.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.8.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.8.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.8.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.8.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.8.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.8.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.8.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.8.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.8.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.9.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.9.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.9.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.9.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.9.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.9.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.9.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.9.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.9.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.9.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.9.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.9.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.9.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.9.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.10.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.10.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.10.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.10.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.10.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.10.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.10.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.10.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.10.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.10.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.10.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.10.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.10.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.10.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.encoder.11.norm1.weight \t torch.Size([768])\n",
      "transformer.encoder.11.norm1.bias \t torch.Size([768])\n",
      "transformer.encoder.11.attn.qkv.weight \t torch.Size([2304, 768])\n",
      "transformer.encoder.11.attn.qkv.bias \t torch.Size([2304])\n",
      "transformer.encoder.11.attn.proj.weight \t torch.Size([768, 768])\n",
      "transformer.encoder.11.attn.proj.bias \t torch.Size([768])\n",
      "transformer.encoder.11.norm2.weight \t torch.Size([768])\n",
      "transformer.encoder.11.norm2.bias \t torch.Size([768])\n",
      "transformer.encoder.11.mlp.fc1.weight \t torch.Size([3072, 768])\n",
      "transformer.encoder.11.mlp.fc1.bias \t torch.Size([3072])\n",
      "transformer.encoder.11.mlp.fc2.0.weight \t torch.Size([3072])\n",
      "transformer.encoder.11.mlp.fc2.0.bias \t torch.Size([3072])\n",
      "transformer.encoder.11.mlp.fc2.2.weight \t torch.Size([768, 3072])\n",
      "transformer.encoder.11.mlp.fc2.2.bias \t torch.Size([768])\n",
      "transformer.fcOut.0.fc1.weight \t torch.Size([768, 768])\n",
      "transformer.fcOut.0.fc1.bias \t torch.Size([768])\n",
      "transformer.fcOut.0.fc2.0.weight \t torch.Size([768])\n",
      "transformer.fcOut.0.fc2.0.bias \t torch.Size([768])\n",
      "transformer.fcOut.0.fc2.2.weight \t torch.Size([768, 768])\n",
      "transformer.fcOut.0.fc2.2.bias \t torch.Size([768])\n",
      "transformer.fcOut.1.weight \t torch.Size([768])\n",
      "transformer.fcOut.1.bias \t torch.Size([768])\n",
      "MLMHead.fc_in.fc1.weight \t torch.Size([768, 768])\n",
      "MLMHead.fc_in.fc1.bias \t torch.Size([768])\n",
      "MLMHead.fc_in.fc2.0.weight \t torch.Size([768])\n",
      "MLMHead.fc_in.fc2.0.bias \t torch.Size([768])\n",
      "MLMHead.fc_in.fc2.2.weight \t torch.Size([512, 768])\n",
      "MLMHead.fc_in.fc2.2.bias \t torch.Size([512])\n",
      "MLMHead.upconv.0.conv1.0.conv.0.weight \t torch.Size([512, 1, 3, 3])\n",
      "MLMHead.upconv.0.conv1.0.conv.1.weight \t torch.Size([512])\n",
      "MLMHead.upconv.0.conv1.0.conv.1.bias \t torch.Size([512])\n",
      "MLMHead.upconv.0.conv1.0.conv.1.running_mean \t torch.Size([512])\n",
      "MLMHead.upconv.0.conv1.0.conv.1.running_var \t torch.Size([512])\n",
      "MLMHead.upconv.0.conv1.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.0.conv1.0.conv.3.weight \t torch.Size([768, 512, 1, 1])\n",
      "MLMHead.upconv.0.conv1.0.conv.4.weight \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.0.conv.4.bias \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.0.conv.4.running_mean \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.0.conv.4.running_var \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.0.conv1.1.conv.0.weight \t torch.Size([768, 1, 3, 3])\n",
      "MLMHead.upconv.0.conv1.1.conv.1.weight \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.1.conv.1.bias \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.1.conv.1.running_mean \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.1.conv.1.running_var \t torch.Size([768])\n",
      "MLMHead.upconv.0.conv1.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.0.conv1.1.conv.3.weight \t torch.Size([384, 768, 1, 1])\n",
      "MLMHead.upconv.0.conv1.1.conv.4.weight \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv1.1.conv.4.bias \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv1.1.conv.4.running_mean \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv1.1.conv.4.running_var \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv1.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.0.conv2.conv.0.weight \t torch.Size([384, 1, 3, 3])\n",
      "MLMHead.upconv.0.conv2.conv.1.weight \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.1.bias \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.1.running_mean \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.1.running_var \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.0.conv2.conv.3.weight \t torch.Size([384, 384, 1, 1])\n",
      "MLMHead.upconv.0.conv2.conv.4.weight \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.4.bias \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.4.running_mean \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.4.running_var \t torch.Size([384])\n",
      "MLMHead.upconv.0.conv2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv1.0.conv.0.weight \t torch.Size([384, 1, 3, 3])\n",
      "MLMHead.upconv.1.conv1.0.conv.1.weight \t torch.Size([384])\n",
      "MLMHead.upconv.1.conv1.0.conv.1.bias \t torch.Size([384])\n",
      "MLMHead.upconv.1.conv1.0.conv.1.running_mean \t torch.Size([384])\n",
      "MLMHead.upconv.1.conv1.0.conv.1.running_var \t torch.Size([384])\n",
      "MLMHead.upconv.1.conv1.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv1.0.conv.3.weight \t torch.Size([512, 384, 1, 1])\n",
      "MLMHead.upconv.1.conv1.0.conv.4.weight \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.0.conv.4.bias \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.0.conv.4.running_mean \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.0.conv.4.running_var \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv1.1.conv.0.weight \t torch.Size([512, 1, 3, 3])\n",
      "MLMHead.upconv.1.conv1.1.conv.1.weight \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.1.conv.1.bias \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.1.conv.1.running_mean \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.1.conv.1.running_var \t torch.Size([512])\n",
      "MLMHead.upconv.1.conv1.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv1.1.conv.3.weight \t torch.Size([256, 512, 1, 1])\n",
      "MLMHead.upconv.1.conv1.1.conv.4.weight \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv1.1.conv.4.bias \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv1.1.conv.4.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv1.1.conv.4.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv1.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv2.conv.0.weight \t torch.Size([256, 1, 3, 3])\n",
      "MLMHead.upconv.1.conv2.conv.1.weight \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.1.bias \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.1.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.1.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.1.conv2.conv.3.weight \t torch.Size([256, 256, 1, 1])\n",
      "MLMHead.upconv.1.conv2.conv.4.weight \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.4.bias \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.4.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.4.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.1.conv2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv1.0.conv.0.weight \t torch.Size([256, 1, 3, 3])\n",
      "MLMHead.upconv.2.conv1.0.conv.1.weight \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.1.bias \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.1.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.1.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv1.0.conv.3.weight \t torch.Size([256, 256, 1, 1])\n",
      "MLMHead.upconv.2.conv1.0.conv.4.weight \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.4.bias \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.4.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.4.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv1.1.conv.0.weight \t torch.Size([256, 1, 3, 3])\n",
      "MLMHead.upconv.2.conv1.1.conv.1.weight \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.1.conv.1.bias \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.1.conv.1.running_mean \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.1.conv.1.running_var \t torch.Size([256])\n",
      "MLMHead.upconv.2.conv1.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv1.1.conv.3.weight \t torch.Size([128, 256, 1, 1])\n",
      "MLMHead.upconv.2.conv1.1.conv.4.weight \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv1.1.conv.4.bias \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv1.1.conv.4.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv1.1.conv.4.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv1.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv2.conv.0.weight \t torch.Size([128, 1, 3, 3])\n",
      "MLMHead.upconv.2.conv2.conv.1.weight \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.1.bias \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.1.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.1.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.2.conv2.conv.3.weight \t torch.Size([128, 128, 1, 1])\n",
      "MLMHead.upconv.2.conv2.conv.4.weight \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.4.bias \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.4.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.4.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.2.conv2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv1.0.conv.0.weight \t torch.Size([128, 1, 3, 3])\n",
      "MLMHead.upconv.3.conv1.0.conv.1.weight \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.1.bias \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.1.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.1.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv1.0.conv.3.weight \t torch.Size([128, 128, 1, 1])\n",
      "MLMHead.upconv.3.conv1.0.conv.4.weight \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.4.bias \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.4.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.4.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv1.1.conv.0.weight \t torch.Size([128, 1, 3, 3])\n",
      "MLMHead.upconv.3.conv1.1.conv.1.weight \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.1.conv.1.bias \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.1.conv.1.running_mean \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.1.conv.1.running_var \t torch.Size([128])\n",
      "MLMHead.upconv.3.conv1.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv1.1.conv.3.weight \t torch.Size([64, 128, 1, 1])\n",
      "MLMHead.upconv.3.conv1.1.conv.4.weight \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv1.1.conv.4.bias \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv1.1.conv.4.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv1.1.conv.4.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv1.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv2.conv.0.weight \t torch.Size([64, 1, 3, 3])\n",
      "MLMHead.upconv.3.conv2.conv.1.weight \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.1.bias \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.1.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.1.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.3.conv2.conv.3.weight \t torch.Size([64, 64, 1, 1])\n",
      "MLMHead.upconv.3.conv2.conv.4.weight \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.4.bias \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.4.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.4.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.3.conv2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv1.0.conv.0.weight \t torch.Size([64, 1, 3, 3])\n",
      "MLMHead.upconv.4.conv1.0.conv.1.weight \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.1.bias \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.1.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.1.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv1.0.conv.3.weight \t torch.Size([64, 64, 1, 1])\n",
      "MLMHead.upconv.4.conv1.0.conv.4.weight \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.4.bias \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.4.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.4.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.0.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv1.1.conv.0.weight \t torch.Size([64, 1, 3, 3])\n",
      "MLMHead.upconv.4.conv1.1.conv.1.weight \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.1.conv.1.bias \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.1.conv.1.running_mean \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.1.conv.1.running_var \t torch.Size([64])\n",
      "MLMHead.upconv.4.conv1.1.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv1.1.conv.3.weight \t torch.Size([32, 64, 1, 1])\n",
      "MLMHead.upconv.4.conv1.1.conv.4.weight \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv1.1.conv.4.bias \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv1.1.conv.4.running_mean \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv1.1.conv.4.running_var \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv1.1.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv2.conv.0.weight \t torch.Size([32, 1, 3, 3])\n",
      "MLMHead.upconv.4.conv2.conv.1.weight \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.1.bias \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.1.running_mean \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.1.running_var \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.1.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.4.conv2.conv.3.weight \t torch.Size([32, 32, 1, 1])\n",
      "MLMHead.upconv.4.conv2.conv.4.weight \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.4.bias \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.4.running_mean \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.4.running_var \t torch.Size([32])\n",
      "MLMHead.upconv.4.conv2.conv.4.num_batches_tracked \t torch.Size([])\n",
      "MLMHead.upconv.5.weight \t torch.Size([2, 32, 3, 3])\n",
      "MLMHead.upconv.5.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in state_dict:\n",
    "        #打印 key value字典\n",
    "        print(param,'\\t',state_dict[param].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
