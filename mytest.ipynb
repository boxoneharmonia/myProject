{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4a81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.net import EventBERTMLMV2\n",
    "from config import config\n",
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001d978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 初始化配置 ---\n",
      "--- 实例化 Backbone 模型 ---\n",
      "\n",
      "--- 准备模拟输入数据 ---\n",
      "\n",
      "--- 执行模型前向传播 ---\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "EventBERTMLMV2                                          [1, 3, 2, 200, 200]       768\n",
      "├─EventImg2TokenV2: 1-1                                 [1, 294, 768]             --\n",
      "│    └─Sequential: 2-1                                  [6, 1536, 7, 7]           --\n",
      "│    │    └─ConvDw: 3-1                                 [6, 32, 100, 100]         150\n",
      "│    │    └─ConvDw: 3-2                                 [6, 128, 50, 50]          4,704\n",
      "│    │    └─ConvDw: 3-3                                 [6, 512, 25, 25]          67,968\n",
      "│    │    └─ConvDw: 3-4                                 [6, 1024, 13, 13]         531,968\n",
      "│    │    └─ConvDw: 3-5                                 [6, 1536, 7, 7]           1,587,200\n",
      "│    └─MLPSwiGLU: 2-2                                   [1, 294, 768]             --\n",
      "│    │    └─Linear: 3-6                                 [1, 294, 4096]            6,295,552\n",
      "│    │    └─Identity: 3-7                               [1, 294, 4096]            --\n",
      "│    │    └─Linear: 3-8                                 [1, 294, 768]             1,573,632\n",
      "├─TransformerV2: 1-2                                    [1, 294, 768]             18,432\n",
      "│    └─Sequential: 2-3                                  [1, 318, 768]             --\n",
      "│    │    └─BlockFT: 3-9                                [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-10                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-11                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-12                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-13                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-14                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-15                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-16                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-17                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-18                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-19                               [1, 318, 768]             9,452,800\n",
      "│    │    └─BlockFT: 3-20                               [1, 318, 768]             9,452,800\n",
      "│    └─Sequential: 2-4                                  [1, 318, 768]             --\n",
      "│    │    └─MLPSwiGLU: 3-21                             [1, 318, 768]             1,181,440\n",
      "│    │    └─LayerNormCompatible: 3-22                   [1, 318, 768]             1,536\n",
      "├─Token2EventImgV2: 1-3                                 [1, 3, 2, 112, 112]       --\n",
      "│    └─MLP_base: 2-5                                    [1, 147, 512]             --\n",
      "│    │    └─Linear: 3-23                                [1, 147, 768]             590,592\n",
      "│    │    └─Identity: 3-24                              [1, 147, 768]             --\n",
      "│    │    └─Sequential: 3-25                            [1, 147, 512]             395,264\n",
      "│    └─Sequential: 2-6                                  [3, 2, 112, 112]          --\n",
      "│    │    └─UpConv: 3-26                                [3, 384, 14, 14]          856,960\n",
      "│    │    └─UpConv: 3-27                                [3, 384, 28, 28]          757,248\n",
      "│    │    └─UpConv: 3-28                                [3, 128, 56, 56]          156,928\n",
      "│    │    └─UpConv: 3-29                                [3, 32, 112, 112]         13,984\n",
      "│    │    └─Conv2d: 3-30                                [3, 2, 112, 112]          578\n",
      "│    │    └─Tanh: 3-31                                  [3, 2, 112, 112]          --\n",
      "=========================================================================================================\n",
      "Total params: 127,468,504\n",
      "Trainable params: 127,468,504\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 4.63\n",
      "=========================================================================================================\n",
      "Input size (MB): 1.92\n",
      "Forward/backward pass size (MB): 796.23\n",
      "Params size (MB): 509.80\n",
      "Estimated Total Size (MB): 1307.95\n",
      "=========================================================================================================\n",
      "\n",
      "--- 测试完成 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 初始化配置 ---\")\n",
    "\n",
    "print(\"--- 实例化 Backbone 模型 ---\")\n",
    "model = EventBERTMLMV2(config)\n",
    "\n",
    "print(\"\\n--- 准备模拟输入数据 ---\")\n",
    "batch_size = 1\n",
    "seq_len = config.max_seq_len # 确保 seq_len <= config.max_seq_len\n",
    "input_channels = 2\n",
    "input_height = 200\n",
    "input_width = 200\n",
    "\n",
    "# 模拟正负事件图像序列\n",
    "# (batch_size, seq_len, C, H, W)\n",
    "mock_x = torch.randn(batch_size, seq_len, input_channels, input_height, input_width)\n",
    "\n",
    "print(\"\\n--- 执行模型前向传播 ---\")\n",
    "summary(model, input_data=[mock_x], verbose=1)\n",
    "\n",
    "print(\"\\n--- 测试完成 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd25e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del mock_x \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d38abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始从 DataLoader 中取出一个批次的数据 ---\n",
      "成功取出一个批次！\n",
      "x_seq 的形状: torch.Size([1, 24, 2, 200, 200])\n",
      "traj_seq 的形状: torch.Size([1, 24, 10])\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from src.dataset import build_dataloader\n",
    "\n",
    "config.task = 'traj'\n",
    "config.batch_size = 1\n",
    "\n",
    "dataloader = build_dataloader(config)\n",
    "\n",
    "# 实例化 DataLoader\n",
    "\n",
    "print(\"\\n--- 开始从 DataLoader 中取出一个批次的数据 ---\")\n",
    "\n",
    "# 从迭代器中获取一个批次\n",
    "try:\n",
    "    x_seq, traj_seq = next(iter(dataloader))\n",
    "    \n",
    "    print(f\"成功取出一个批次！\")\n",
    "    print(f\"x_seq 的形状: {x_seq.shape}\")\n",
    "    print(f\"traj_seq 的形状: {traj_seq.shape}\")\n",
    "\n",
    "        \n",
    "except StopIteration:\n",
    "    print(\"错误： DataLoader 为空，无法取出数据。请检查数据集路径和内容。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import torch\n",
    "print(accelerate.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.bfloat16)\n",
    "print(a, a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def KL(p,q):\n",
    "    # p,q 为两个 list，表示对应取值的概率 且 sum(p) == 1 ，sum(q) == 1\n",
    "    return sum(_p*math.log(_p/_q) for (_p,_q) in zip(p,q) if _p != 0 )\n",
    "\n",
    "P = [0.2, 0.4, 0.4]\n",
    "Q = [0.2, 0.4, 0.4]\n",
    "\n",
    "print(KL(P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from src.net import build_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# config.use_cuda = False\n",
    "\n",
    "if torch.cuda.is_available() and config.use_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_num_threads(8) \n",
    "\n",
    "model = build_model(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "weight_path = os.path.join(config.weight_dir, \"event_bert_mlm\" + \".pth\")\n",
    "if os.path.exists(weight_path):\n",
    "    model.load_state_dict(torch.load(weight_path, map_location='cpu'), strict=False)\n",
    "    logger.info(f\"Loading pretrained weights from {weight_path}\")\n",
    "else:\n",
    "    logger.warning(f\"Pretrained weights not found at {weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "captured_outputs = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    一个前向钩子函数。\n",
    "    每次模块(module)的forward被调用后，此函数会被触发。\n",
    "    'output' 就是我们想要的 eventImg2Token 的输出结果。\n",
    "    \"\"\"\n",
    "    # 我们将输出从GPU移到CPU并分离计算图，以防内存泄漏\n",
    "    captured_outputs.append(output.detach().cpu())\n",
    "\n",
    "target_module = model.eventImg2Token\n",
    "handle = target_module.register_forward_hook(forward_hook)    \n",
    "\n",
    "num_test_loops = 20  # 我们测试20个不同的随机输入\n",
    "with torch.no_grad(): # 确保不计算梯度\n",
    "    for _ in tqdm(range(num_test_loops), desc=\"Diagnosing Collapse\"):\n",
    "        # 在每次循环中创建全新的随机输入\n",
    "        mock_x_pos = torch.randn(1, config.max_seq_len, 3, 200, 200) *2 -1\n",
    "        mock_x_neg = torch.randn(1, config.max_seq_len, 3, 200, 200) *2 -1\n",
    "        \n",
    "        # 正常调用模型。当我们调用它时，钩子会自动被触发\n",
    "        model(mock_x_pos.to(device), mock_x_neg.to(device), config.mask_probability)\n",
    "\n",
    "# --- 分析捕获到的数据 ---\n",
    "if captured_outputs:\n",
    "    # 将所有捕获到的输出张量拼接在一起\n",
    "    # 每个输出的形状是 (1, S, token_len)，拼接后是 (num_test_loops, S, token_len)\n",
    "    all_outputs = torch.cat(captured_outputs, dim=0)\n",
    "\n",
    "    # 对logits进行softmax，转换为概率分布\n",
    "    all_probs = torch.softmax(all_outputs, dim=-1)\n",
    "\n",
    "    # 计算在“样本”维度(dim=0)上的标准差\n",
    "    stdev_across_samples = all_probs.std(dim=0)\n",
    "    mean_stdev = stdev_across_samples.mean().item()\n",
    "\n",
    "    logger.info(f\"--- Diagnosis Result ---\")\n",
    "    logger.info(f\"Mean Standard Deviation of softmax outputs across {num_test_loops} random inputs: {mean_stdev:.8f}\")\n",
    "\n",
    "    if mean_stdev < 1e-4: # 一个非常低的阈值\n",
    "        logger.error(\"!!! CRITICAL: Representation Collapse DETECTED.\")\n",
    "        logger.info(\"The model produces nearly identical probability distributions for different random inputs.\")\n",
    "        \n",
    "        # 亲眼看看证据\n",
    "        logger.info(\"Displaying softmax probability of the first token from the first 3 random inputs:\")\n",
    "        for i in range(min(3, num_test_loops)):\n",
    "            # 打印第一个样本，第一个token，前10个值的概率\n",
    "            print(f\"Random Input {i+1}: {all_probs[i, 0, :10]}...\")\n",
    "    else:\n",
    "        logger.info(\"OK: No obvious sign of representation collapse. The model produces diverse outputs.\")\n",
    "else:\n",
    "    logger.warning(\"Hook did not capture any outputs.\")\n",
    "\n",
    "# --- 4. 事后清理：移除钩子 ---\n",
    "# 这是一个好习惯，防止钩子在后续代码中继续产生影响\n",
    "handle.remove()\n",
    "logger.info(\"Hook removed.\")\n",
    "\n",
    "del model\n",
    "del mock_x_pos\n",
    "del mock_x_neg \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "num_vectors = all_outputs.size(0) * all_outputs.size(1)\n",
    "token_dimension = all_outputs.size(2)\n",
    "\n",
    "all_vectors_flat = all_outputs.view(num_vectors, token_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0606e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_tsne(vectors: torch.Tensor):\n",
    "    \"\"\"\n",
    "    使用t-SNE对向量进行二维可视化。\n",
    "    \"\"\"\n",
    "    logger.info(\"--- t-SNE Visualization ---\")\n",
    "    logger.info(\"Running t-SNE... this may take a moment.\")\n",
    "    \n",
    "    # t-SNE需要numpy数组\n",
    "    vectors_np = vectors.cpu().numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=15, learning_rate='auto', init='random')\n",
    "    vectors_2d = tsne.fit_transform(vectors_np)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6)\n",
    "    plt.title(\"t-SNE Visualization of Output Vectors\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    # 保存图像而不是显示\n",
    "    # plt.savefig(\"tsne_visualization.png\")\n",
    "    # logger.info(\"t-SNE visualization saved to tsne_visualization.png\")\n",
    "\n",
    "visualize_tsne(all_vectors_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_cosine_similarity(vectors: torch.Tensor):\n",
    "    \"\"\"\n",
    "    分析一批向量的平均余弦相似度。\n",
    "    Args:\n",
    "        vectors (torch.Tensor): 形状为 (N, D) 的向量，N是样本数，D是向量维度。\n",
    "    \"\"\"\n",
    "    # 1. 对向量进行L2归一化\n",
    "    vectors_normalized = F.normalize(vectors, p=2, dim=1)\n",
    "    \n",
    "    # 2. 计算所有成对的余弦相似度\n",
    "    # (N, D) x (D, N) -> (N, N) 的相似度矩阵\n",
    "    cosine_matrix = torch.matmul(vectors_normalized, vectors_normalized.t())\n",
    "    \n",
    "    # 3. 提取上三角部分（不包括对角线），计算平均值\n",
    "    # 对角线上的值总是1（自己和自己的相似度），需要排除\n",
    "    N = vectors.size(0)\n",
    "    # 创建一个上三角掩码\n",
    "    mask = torch.triu(torch.ones(N, N), diagonal=1).bool()\n",
    "    # 提取所有成对相似度值\n",
    "    pairwise_similarities = cosine_matrix[mask]\n",
    "    \n",
    "    # 4. 计算平均值和标准差\n",
    "    avg_cosine_sim = pairwise_similarities.mean().item()\n",
    "    std_cosine_sim = pairwise_similarities.std().item()\n",
    "\n",
    "    logger.info(f\"--- Cosine Similarity Analysis ---\")\n",
    "    logger.info(f\"Average pairwise cosine similarity: {avg_cosine_sim:.6f}\")\n",
    "    logger.info(f\"Std dev of cosine similarity: {std_cosine_sim:.6f}\")\n",
    "\n",
    "    if avg_cosine_sim > 0.99: # 设置一个非常高的阈值\n",
    "        logger.error(\"!!! CRITICAL: Representation Collapse DETECTED via Cosine Similarity.\")\n",
    "        logger.error(\"Vectors are pointing in nearly the same direction.\")\n",
    "    else:\n",
    "        logger.info(\"OK: Cosine similarity analysis shows no sign of collapse.\")\n",
    "\n",
    "analyze_cosine_similarity(all_vectors_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from transformers.optimization import get_scheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "tensor = torch.FloatTensor([[4]]) # build a tensor\n",
    "x = Variable(tensor, requires_grad=True) # build a variable, usually for compute gradients\n",
    "\n",
    "scheduler_name = 'polynomial'\n",
    "scheduler_specific_kwargs = {}\n",
    "if scheduler_name == 'cosine_with_restarts':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'num_cycles': 2,\n",
    "    }\n",
    "elif scheduler_name == 'cosine_with_min_lr':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'min_lr': 0.1\n",
    "    }\n",
    "elif scheduler_name == 'polynomial':\n",
    "    scheduler_specific_kwargs = {\n",
    "        'power': 2.5\n",
    "    }\n",
    "steps = 100\n",
    "warmup = 0.1\n",
    "optimizer2 = torch.optim.Adam([x], lr=0.1)\n",
    "scheduler2 = get_scheduler(\n",
    "        name=scheduler_name,\n",
    "        optimizer=optimizer2,\n",
    "        num_warmup_steps=int(steps*warmup),\n",
    "        num_training_steps=steps*2,\n",
    "        scheduler_specific_kwargs=scheduler_specific_kwargs\n",
    ")\n",
    "\n",
    "learning_rates2 = []\n",
    "\n",
    "for step in range(steps):\n",
    "    learning_rates2.append(optimizer2.param_groups[0]['lr'])\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    outputs2 = torch.pow((x - 5), 2)\n",
    "    loss2 = outputs2\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Plotting the learning rates\n",
    "plt.plot(range(steps), learning_rates2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(scheduler_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc0e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import config\n",
    "import os\n",
    "weight_path = os.path.join(config.weight_dir, \"event_bert_mlm.pth\")\n",
    "state_dict = torch.load(weight_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in state_dict:\n",
    "        #打印 key value字典\n",
    "        print(param,'\\t',state_dict[param].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c760897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinov2Model(\n",
      "  (embeddings): Dinov2Embeddings(\n",
      "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
      "      (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): Dinov2Encoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x Dinov2Layer(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attention): Dinov2Attention(\n",
      "          (attention): Dinov2SelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output): Dinov2SelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (layer_scale1): Dinov2LayerScale()\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Dinov2MLP(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_scale2): Dinov2LayerScale()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Dinov2Config, Dinov2Model\n",
    "\n",
    "# Initializing a Dinov2 dinov2-base-patch16-224 style configuration\n",
    "configuration = Dinov2Config()\n",
    "\n",
    "# Initializing a model (with random weights) from the dinov2-base-patch16-224 style configuration\n",
    "model = Dinov2Model(configuration)\n",
    "\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
