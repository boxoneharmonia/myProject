{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "INFO:__main__:Starting training with bf16 mixed precision.\n",
      "INFO:__main__:Random seed value: 42\n",
      "INFO:__main__:Weight saved at: ./weight\n",
      "INFO:__main__:Net created successfully.\n",
      "INFO:__main__:Model parameters: 98182680\n",
      "INFO:__main__:Model trainable parameters: 98182680\n",
      "INFO:__main__:Optimizer: adamw, Learning rate: 1e-05\n",
      "INFO:__main__:Loading pretrained weights from ./weight/event_bert_mlm.pth\n",
      "INFO:src.dataset:Dataset initialized with 42222 sequences from ./dataset/train.\n",
      "INFO:__main__:Train dataloader created with 2345 batches.\n",
      "INFO:__main__:polynomial scheduler created with 50 epochs.\n",
      "/home/box/.local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/home/box/.local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Epoch: 1, Batch: [ 781/781 ], LR: 0.00000000, Loss: 0.528278 (Avg: 0.537041), Elapsed: 330.95 ms\n",
      "/home/box/.local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Epoch: 2, Batch: [ 781/781 ], LR: 0.00000400, Loss: 0.535359 (Avg: 0.539858), Elapsed: 329.29 ms\n",
      "Epoch: 3, Batch: [ 781/781 ], LR: 0.00000800, Loss: 0.539616 (Avg: 0.537989), Elapsed: 323.98 ms\n",
      "Epoch: 4, Batch: [ 781/781 ], LR: 0.00000979, Loss: 0.545248 (Avg: 0.539686), Elapsed: 334.93 ms\n",
      "Epoch: 5, Batch: [ 781/781 ], LR: 0.00000938, Loss: 0.489814 (Avg: 0.538988), Elapsed: 331.77 ms\n",
      "Epoch: 6, Batch: [ 781/781 ], LR: 0.00000898, Loss: 0.548906 (Avg: 0.539684), Elapsed: 333.71 ms\n",
      "Epoch: 7, Batch: [ 781/781 ], LR: 0.00000859, Loss: 0.502726 (Avg: 0.540277), Elapsed: 330.59 ms\n",
      "Epoch: 8, Batch: [ 781/781 ], LR: 0.00000821, Loss: 0.497674 (Avg: 0.540681), Elapsed: 332.98 ms\n",
      "Epoch: 9, Batch: [ 781/781 ], LR: 0.00000784, Loss: 0.509575 (Avg: 0.539909), Elapsed: 336.34 ms\n",
      "Epoch: 10, Batch: [ 781/781 ], LR: 0.00000747, Loss: 0.547586 (Avg: 0.538942), Elapsed: 327.02 ms\n",
      "Epoch: 11, Batch: [ 781/781 ], LR: 0.00000712, Loss: 0.531856 (Avg: 0.538607), Elapsed: 329.52 ms\n",
      "Epoch: 12, Batch: [ 781/781 ], LR: 0.00000677, Loss: 0.502154 (Avg: 0.538260), Elapsed: 329.61 ms\n",
      "Epoch: 13, Batch: [ 781/781 ], LR: 0.00000643, Loss: 0.432555 (Avg: 0.538995), Elapsed: 333.00 ms\n",
      "Epoch: 14, Batch: [ 781/781 ], LR: 0.00000611, Loss: 0.561000 (Avg: 0.538462), Elapsed: 325.41 ms\n",
      "Epoch: 15, Batch: [ 781/781 ], LR: 0.00000578, Loss: 0.459470 (Avg: 0.537960), Elapsed: 332.42 ms\n",
      "Epoch: 16, Batch: [ 781/781 ], LR: 0.00000547, Loss: 0.523267 (Avg: 0.537808), Elapsed: 330.27 ms\n",
      "Epoch: 17, Batch: [ 781/781 ], LR: 0.00000517, Loss: 0.553714 (Avg: 0.536999), Elapsed: 333.32 ms\n",
      "Epoch: 18, Batch: [ 781/781 ], LR: 0.00000488, Loss: 0.505667 (Avg: 0.537042), Elapsed: 336.13 ms\n",
      "Epoch: 19, Batch: [ 781/781 ], LR: 0.00000459, Loss: 0.505703 (Avg: 0.538943), Elapsed: 333.92 ms\n",
      "Epoch: 20, Batch: [ 781/781 ], LR: 0.00000431, Loss: 0.561255 (Avg: 0.536753), Elapsed: 328.29 ms\n",
      "Epoch: 21, Batch: [ 781/781 ], LR: 0.00000405, Loss: 0.540124 (Avg: 0.538133), Elapsed: 331.77 ms\n",
      "[rank2]:[E801 16:10:30.811267919 ProcessGroupNCCL.cpp:632] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=143162, OpType=ALLREDUCE, NumelIn=365666, NumelOut=365666, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.\n",
      "[rank2]:[E801 16:10:30.811548653 ProcessGroupNCCL.cpp:2271] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 143162 PG status: last enqueued work: 143176, last completed work: 143161\n",
      "[rank2]:[E801 16:10:30.811565263 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.\n",
      "[rank2]:[E801 16:10:30.811706010 ProcessGroupNCCL.cpp:2106] [PG ID 0 PG GUID 0(default_pg) Rank 2] First PG on this rank to signal dumping.\n",
      "[rank0]:[E801 16:10:30.816032321 ProcessGroupNCCL.cpp:632] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=143162, OpType=ALLREDUCE, NumelIn=365666, NumelOut=365666, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.\n",
      "[rank0]:[E801 16:10:30.816309944 ProcessGroupNCCL.cpp:2271] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 143162 PG status: last enqueued work: 143176, last completed work: 143161\n",
      "[rank0]:[E801 16:10:30.816329135 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.\n",
      "[rank0]:[E801 16:10:30.816420179 ProcessGroupNCCL.cpp:2106] [PG ID 0 PG GUID 0(default_pg) Rank 0] First PG on this rank to signal dumping.\n",
      "[rank0]:[E801 16:10:30.327616964 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 143176, last completed NCCL work: 143161.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. \n",
      "[rank2]:[E801 16:10:30.327646155 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 143176, last completed NCCL work: 143161.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. \n",
      "[rank1]:[E801 16:10:30.327748080 ProcessGroupNCCL.cpp:1685] [PG ID 0 PG GUID 0(default_pg) Rank 1] Observed flight recorder dump signal from another rank via TCPStore.\n",
      "[rank0]:[E801 16:10:30.328102397 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1\n",
      "[rank2]:[E801 16:10:30.328102407 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1\n",
      "[rank1]:[E801 16:10:30.328253534 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from  rank 0 and we will try our best to dump the debug info. Last enqueued NCCL work: 143161, last completed NCCL work: 143161.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. \n",
      "[rank1]:[E801 16:10:30.328544287 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1\n",
      "[rank2]:[F801 16:18:30.329054596 ProcessGroupNCCL.cpp:1557] [PG ID 0 PG GUID 0(default_pg) Rank 2] [PG ID 0 PG GUID 0(default_pg) Rank 2] Terminating the process after attempting to dump debug info, due to collective timeout or exception.\n",
      "[rank0]:[F801 16:18:30.329536959 ProcessGroupNCCL.cpp:1557] [PG ID 0 PG GUID 0(default_pg) Rank 0] [PG ID 0 PG GUID 0(default_pg) Rank 0] Terminating the process after attempting to dump debug info, due to collective timeout or exception.\n",
      "[rank1]:[F801 16:18:30.329538589 ProcessGroupNCCL.cpp:1557] [PG ID 0 PG GUID 0(default_pg) Rank 1] [PG ID 0 PG GUID 0(default_pg) Rank 1] Terminating the process after attempting to dump debug info, due to collective timeout or exception.\n",
      "W0801 16:19:45.739000 1958126 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1958318 closing signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!accelerate launch train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
